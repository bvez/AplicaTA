{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Para este laboratorio, hemos decidido usar un dataset con los tweets que realizaron las personas frente a Apple</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura del archivo de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"Apple-Twitter-Sentiment-DFE.csv\", header=0,encoding = 'utf_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment:confidence</th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>query</th>\n",
       "      <th>sentiment_gold</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>623495590</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>12/12/14 2:23</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6783</td>\n",
       "      <td>Tue Dec 02 01:44:36 +0000 2014</td>\n",
       "      <td>5.400000e+17</td>\n",
       "      <td>#AAPL OR @Apple</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @thehill: Justice Department cites 18th cen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2265</th>\n",
       "      <td>623497795</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>12/11/14 22:25</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6861</td>\n",
       "      <td>Fri Dec 05 23:39:50 +0000 2014</td>\n",
       "      <td>5.410000e+17</td>\n",
       "      <td>#AAPL OR @Apple</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BREAKING: #NYC #EricGarnet protestors occupy @...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3351</th>\n",
       "      <td>623498881</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>12/12/14 8:52</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6708</td>\n",
       "      <td>Tue Dec 09 01:26:29 +0000 2014</td>\n",
       "      <td>5.420000e+17</td>\n",
       "      <td>#AAPL OR @Apple</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@TeamCavuto @Apple Most of these stooges have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959</th>\n",
       "      <td>623497484</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>12/12/14 2:25</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Fri Dec 05 12:31:10 +0000 2014</td>\n",
       "      <td>5.410000e+17</td>\n",
       "      <td>#AAPL OR @Apple</td>\n",
       "      <td>NaN</td>\n",
       "      <td>iPhon6 rivals by Samsung, LG,HTC suffering del...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1662</th>\n",
       "      <td>623497186</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>12/12/14 8:43</td>\n",
       "      <td>1</td>\n",
       "      <td>0.7079</td>\n",
       "      <td>Thu Dec 04 21:27:52 +0000 2014</td>\n",
       "      <td>5.410000e+17</td>\n",
       "      <td>#AAPL OR @Apple</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @OneRepublic: Studio at 45,000 ft.  One out...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       _unit_id  _golden _unit_state  _trusted_judgments _last_judgment_at  \\\n",
       "77    623495590    False   finalized                   3     12/12/14 2:23   \n",
       "2265  623497795    False   finalized                   3    12/11/14 22:25   \n",
       "3351  623498881    False   finalized                   3     12/12/14 8:52   \n",
       "1959  623497484    False   finalized                   3     12/12/14 2:25   \n",
       "1662  623497186    False   finalized                   3     12/12/14 8:43   \n",
       "\n",
       "     sentiment  sentiment:confidence                            date  \\\n",
       "77           3                0.6783  Tue Dec 02 01:44:36 +0000 2014   \n",
       "2265         3                0.6861  Fri Dec 05 23:39:50 +0000 2014   \n",
       "3351         3                0.6708  Tue Dec 09 01:26:29 +0000 2014   \n",
       "1959         3                1.0000  Fri Dec 05 12:31:10 +0000 2014   \n",
       "1662         1                0.7079  Thu Dec 04 21:27:52 +0000 2014   \n",
       "\n",
       "                id            query sentiment_gold  \\\n",
       "77    5.400000e+17  #AAPL OR @Apple            NaN   \n",
       "2265  5.410000e+17  #AAPL OR @Apple            NaN   \n",
       "3351  5.420000e+17  #AAPL OR @Apple            NaN   \n",
       "1959  5.410000e+17  #AAPL OR @Apple            NaN   \n",
       "1662  5.410000e+17  #AAPL OR @Apple            NaN   \n",
       "\n",
       "                                                   text  \n",
       "77    RT @thehill: Justice Department cites 18th cen...  \n",
       "2265  BREAKING: #NYC #EricGarnet protestors occupy @...  \n",
       "3351  @TeamCavuto @Apple Most of these stooges have ...  \n",
       "1959  iPhon6 rivals by Samsung, LG,HTC suffering del...  \n",
       "1662  RT @OneRepublic: Studio at 45,000 ft.  One out...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza de columnas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos las columnas que no aportan informacion en cuanto al sentimiento dentro del texto. Estas columnas son las fechas, identificadores y estados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_golden</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment:confidence</th>\n",
       "      <th>sentiment_gold</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3001</th>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>my phone keeps restarting on its own @apple do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2536</th>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6510</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Die ins staged @Macys and @apple gives me food...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6544</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@apple goes red for aids awareness #ProductRED...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2225</th>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.7201</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Will Apple Pay Consumers $1 Billion For Deleti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051</th>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6631</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Shipping Estimates for 27-Inch Retina iMac Imp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      _golden  _trusted_judgments sentiment  sentiment:confidence  \\\n",
       "3001    False                   3         1                1.0000   \n",
       "2536    False                   3         1                0.6510   \n",
       "526     False                   3         5                0.6544   \n",
       "2225    False                   4         3                0.7201   \n",
       "1051    False                   3         3                0.6631   \n",
       "\n",
       "     sentiment_gold                                               text  \n",
       "3001            NaN  my phone keeps restarting on its own @apple do...  \n",
       "2536            NaN  Die ins staged @Macys and @apple gives me food...  \n",
       "526             NaN  @apple goes red for aids awareness #ProductRED...  \n",
       "2225            NaN  Will Apple Pay Consumers $1 Billion For Deleti...  \n",
       "1051            NaN  Shipping Estimates for 27-Inch Retina iMac Imp...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df.drop([\"_unit_id\",\"_unit_state\",\"date\",\"id\",\"query\",\"_last_judgment_at\"],axis=1)\n",
    "df2.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separacion de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separamos la columna \"sentiment\" que es el dato final que queremos lograr. Además dividimos los datos de forma que el 80% de los datos servirá para entrenar y el 20% restante servirá para probar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = df2.drop(['sentiment'],axis=1)\n",
    "y_all = df2['sentiment']\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "num_test = 0.20\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=num_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "825     @jakeflem @Apple That sucks, don't know what i...\n",
      "1842    Omg omg I just paid a kinder Bueno w @apple #p...\n",
      "1613    RT @OneRepublic: Studio at 45,000 ft.  One out...\n",
      "763     #iPhone6S Rumors Point To A CuriousåÊEarly 201...\n",
      "1182                    11am and 20% battery. Die, @apple\n",
      "3455    RT @TeamCavuto: Protesters stage #DieIn protes...\n",
      "1242                   @Samsung is way better than @Apple\n",
      "260     Every student deserves a chance @apple #connec...\n",
      "873     .@Apple CEO @tim_cook visits @WhiteHouse http:...\n",
      "1910    Apple @apple #thinkdifferent http://t.co/4jk0Y...\n",
      "3784    Please @apple, make the green button do what i...\n",
      "3480    @panic @jpetersen good call I thought @Apple w...\n",
      "3318    RT @TeamCavuto: Protesters stage #DieIn protes...\n",
      "2725    Mark #Zuckerberg defends free #Facebook, fires...\n",
      "2769    #Music is always part of my life, and everyday...\n",
      "381     RT @SparkingAri: @ohiloveariana damn I wish I ...\n",
      "194     @QuentinNield @Apple I need to reinstall on my...\n",
      "3577    @apple #ios8 The lack of true keyboard integra...\n",
      "608     @nickruz4 @Apple I can't I just got a new phon...\n",
      "1840    @apple #macos #yosemite is a buggy piece of sh...\n",
      "3638    Hey @Apple the keyboard on my phone is all sor...\n",
      "3288    #Apple 'hoarding' #iPhone6 supply for its own ...\n",
      "3706    First @Apple employee: The remarkable odyssey ...\n",
      "2800          .@apple fix yourself http://t.co/l1VsGFaQib\n",
      "1819    @juji246 @stephanies0601 @BradPaisley @Applebe...\n",
      "1999    RT @tomlindberg99 'Congrats to @Apple, Mary So...\n",
      "2409    THE WORLD'S FIRST MURDER VIA FACETIME THANK YO...\n",
      "3091    #Samsung: How Will The #Galaxy #Note5 Improve ...\n",
      "1137    iPhone chargers can suck my ass @apple #disapp...\n",
      "1390    @apple I thought I was taking my selfie to a n...\n",
      "                              ...                        \n",
      "2851    @apple I have been on hold for 30 minutes than...\n",
      "590     @Apple patents active fall protection system t...\n",
      "3051    Apple Inc. To Open 500 Stores In India http://...\n",
      "1027    Early Movers: ANF, TJX, TGT, LYV, #AAPL &amp; ...\n",
      "3005    Reminiscing on the first low-cost computer whi...\n",
      "3454    @TeamCavuto @charliekirk11 @Apple what did Mac...\n",
      "1455    RT @OneRepublic: Studio at 45,000 ft.  One out...\n",
      "2113    hey @apple could you add an option in mail to ...\n",
      "792     CNBCTV:  #Cramer: #Apple has no real competiti...\n",
      "525     contact @Apple RT @mrstiffanyanne anybody that...\n",
      "1890                    @Kate3015 @Apple get the new sony\n",
      "337     TheStreet:  #Android Phones to Dominate Throug...\n",
      "3723    @carlquintanilla Thanks for all the #AAPL cove...\n",
      "1457    RT @OneRepublic: Studio at 45,000 ft.  One out...\n",
      "2824          @CarlaBarlaCakes @MoBiggaa @Apple snapchat?\n",
      "1475    RT @OneRepublic: Studio at 45,000 ft.  One out...\n",
      "3659    @Apple reveals its Best of 2014 app list- Elev...\n",
      "3845    media reports say that @Apple is hiring pros f...\n",
      "61                      @Quailman_ @Apple u was bored huh\n",
      "1401    #Apple: Kantar's Data Points To A Huge Decembe...\n",
      "2392    @swerviinnn @Apple maybe stop being a whore fo...\n",
      "2900    @apple is laughing its pants off with the scho...\n",
      "1617    RT @OneRepublic: Studio at 45,000 ft.  One out...\n",
      "2249    Apple: Facing A Challenge From Google's Androi...\n",
      "3722    RT @DecodedFashion: .@Apple is reportedly hiri...\n",
      "3025    wtf is my conputer doing I left it alone for f...\n",
      "2030    Hey @apple while your reading list on safari a...\n",
      "917     More Evidence That #Apple, Inc. Needs to Refre...\n",
      "2017    3 Reasons #Apple Is a Potential Warren Buffett...\n",
      "496     Results of our' #Mobility PilotHouse program. ...\n",
      "Name: text, Length: 3108, dtype: object\n",
      "Omg omg I just paid a kinder Bueno w @apple #pay at walgreens awesome experience @ Walgreens http://t.co/f2q6vMOjmG\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "titles =np.array(X_train[\"text\"])\n",
    "print(X_train[\"text\"])\n",
    "print(titles[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "text = \" \".join(titles) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"@jakeflem @Apple That sucks, don't know what it cost in dollars but for us that was å£400 :(\",\n",
       "       'Omg omg I just paid a kinder Bueno w @apple #pay at walgreens awesome experience @ Walgreens http://t.co/f2q6vMOjmG',\n",
       "       'RT @OneRepublic: Studio at 45,000 ft.  One outlet,  4 computers.  @Apple we need the batteries of the future NoW!!!! http://t.co/astp9x6KET',\n",
       "       ...,\n",
       "       'More Evidence That #Apple, Inc. Needs to Refresh #AppleTV | The Motley Fool #aapl http://t.co/UGec9C70Mr',\n",
       "       \"3 Reasons #Apple Is a Potential Warren Buffett Stock and 1 Reason It Isn't | The Motley Fool #aapl http://t.co/BnSCg3I2yo\",\n",
       "       \"Results of our' #Mobility PilotHouse program. @ATT  @Cisco @Apple see which had the most loyal customers. http://t.co/FAyv72N0xt\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=titles\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "txd_matrix = vectorizer.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3108, 7365)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txd_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 188)\t1\n",
      "  (0, 6965)\t1\n",
      "  (0, 6769)\t1\n",
      "  (0, 2659)\t1\n",
      "  (0, 1214)\t1\n",
      "  (0, 2035)\t1\n",
      "  (0, 3358)\t1\n",
      "  (0, 1637)\t1\n",
      "  (0, 3529)\t1\n",
      "  (0, 7032)\t1\n",
      "  (0, 3771)\t1\n",
      "  (0, 2038)\t1\n",
      "  (0, 6159)\t1\n",
      "  (0, 6382)\t2\n",
      "  (0, 698)\t1\n",
      "  (0, 3574)\t1\n",
      "  (1, 2412)\t1\n",
      "  (1, 1498)\t1\n",
      "  (1, 3224)\t1\n",
      "  (1, 2388)\t1\n",
      "  (1, 858)\t1\n",
      "  (1, 6939)\t2\n",
      "  (1, 805)\t1\n",
      "  (1, 4853)\t1\n",
      "  (1, 1193)\t1\n",
      "  :\t:\n",
      "  (3106, 428)\t1\n",
      "  (3106, 6386)\t1\n",
      "  (3106, 1498)\t1\n",
      "  (3106, 3224)\t1\n",
      "  (3106, 3529)\t1\n",
      "  (3106, 698)\t1\n",
      "  (3107, 2493)\t1\n",
      "  (3107, 4044)\t1\n",
      "  (3107, 4936)\t1\n",
      "  (3107, 4317)\t1\n",
      "  (3107, 5460)\t1\n",
      "  (3107, 809)\t1\n",
      "  (3107, 5104)\t1\n",
      "  (3107, 1442)\t1\n",
      "  (3107, 1729)\t1\n",
      "  (3107, 3013)\t1\n",
      "  (3107, 7046)\t1\n",
      "  (3107, 4757)\t1\n",
      "  (3107, 5710)\t1\n",
      "  (3107, 4352)\t1\n",
      "  (3107, 4655)\t1\n",
      "  (3107, 6386)\t1\n",
      "  (3107, 1498)\t1\n",
      "  (3107, 3224)\t1\n",
      "  (3107, 698)\t1\n"
     ]
    }
   ],
   "source": [
    "print (txd_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "txd_matrix = CountVectorizer(binary=True).fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 188)\t1\n",
      "  (0, 6965)\t1\n",
      "  (0, 6769)\t1\n",
      "  (0, 2659)\t1\n",
      "  (0, 1214)\t1\n",
      "  (0, 2035)\t1\n",
      "  (0, 3358)\t1\n",
      "  (0, 1637)\t1\n",
      "  (0, 3529)\t1\n",
      "  (0, 7032)\t1\n",
      "  (0, 3771)\t1\n",
      "  (0, 2038)\t1\n",
      "  (0, 6159)\t1\n",
      "  (0, 6382)\t1\n",
      "  (0, 698)\t1\n",
      "  (0, 3574)\t1\n",
      "  (1, 2412)\t1\n",
      "  (1, 1498)\t1\n",
      "  (1, 3224)\t1\n",
      "  (1, 2388)\t1\n",
      "  (1, 858)\t1\n",
      "  (1, 6939)\t1\n",
      "  (1, 805)\t1\n",
      "  (1, 4853)\t1\n",
      "  (1, 1193)\t1\n",
      "  :\t:\n",
      "  (3106, 428)\t1\n",
      "  (3106, 6386)\t1\n",
      "  (3106, 1498)\t1\n",
      "  (3106, 3224)\t1\n",
      "  (3106, 3529)\t1\n",
      "  (3106, 698)\t1\n",
      "  (3107, 2493)\t1\n",
      "  (3107, 4044)\t1\n",
      "  (3107, 4936)\t1\n",
      "  (3107, 4317)\t1\n",
      "  (3107, 5460)\t1\n",
      "  (3107, 809)\t1\n",
      "  (3107, 5104)\t1\n",
      "  (3107, 1442)\t1\n",
      "  (3107, 1729)\t1\n",
      "  (3107, 3013)\t1\n",
      "  (3107, 7046)\t1\n",
      "  (3107, 4757)\t1\n",
      "  (3107, 5710)\t1\n",
      "  (3107, 4352)\t1\n",
      "  (3107, 4655)\t1\n",
      "  (3107, 6386)\t1\n",
      "  (3107, 1498)\t1\n",
      "  (3107, 3224)\t1\n",
      "  (3107, 698)\t1\n"
     ]
    }
   ],
   "source": [
    "print(txd_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_matrix = TfidfVectorizer(use_idf=False).fit_transform(text)\n",
    "tfidf_matrix = TfidfVectorizer().fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3108, 7351)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2123)\t0.218217890236\n",
      "  (0, 3669)\t0.218217890236\n",
      "  (0, 1475)\t0.436435780472\n",
      "  (0, 3198)\t0.436435780472\n",
      "  (0, 6083)\t0.218217890236\n",
      "  (0, 4401)\t0.218217890236\n",
      "  (0, 1537)\t0.218217890236\n",
      "  (0, 3494)\t0.218217890236\n",
      "  (0, 2779)\t0.218217890236\n",
      "  (0, 3333)\t0.218217890236\n",
      "  (0, 6056)\t0.218217890236\n",
      "  (0, 677)\t0.218217890236\n",
      "  (0, 5644)\t0.218217890236\n",
      "  (0, 7127)\t0.218217890236\n",
      "  (0, 6079)\t0.218217890236\n",
      "  (1, 3934)\t0.229415733871\n",
      "  (1, 6373)\t0.229415733871\n",
      "  (1, 6367)\t0.229415733871\n",
      "  (1, 525)\t0.229415733871\n",
      "  (1, 7020)\t0.229415733871\n",
      "  (1, 6464)\t0.229415733871\n",
      "  (1, 5970)\t0.229415733871\n",
      "  (1, 4397)\t0.229415733871\n",
      "  (1, 5935)\t0.229415733871\n",
      "  (1, 2328)\t0.229415733871\n",
      "  :\t:\n",
      "  (3106, 900)\t0.22360679775\n",
      "  (3106, 4469)\t0.22360679775\n",
      "  (3106, 6542)\t0.22360679775\n",
      "  (3106, 521)\t0.22360679775\n",
      "  (3106, 654)\t0.22360679775\n",
      "  (3106, 6942)\t0.22360679775\n",
      "  (3106, 3525)\t0.22360679775\n",
      "  (3106, 4623)\t0.22360679775\n",
      "  (3106, 1475)\t0.22360679775\n",
      "  (3106, 3198)\t0.22360679775\n",
      "  (3106, 3333)\t0.22360679775\n",
      "  (3106, 677)\t0.22360679775\n",
      "  (3106, 5644)\t0.22360679775\n",
      "  (3107, 3503)\t0.288675134595\n",
      "  (3107, 2854)\t0.288675134595\n",
      "  (3107, 1361)\t0.288675134595\n",
      "  (3107, 7144)\t0.288675134595\n",
      "  (3107, 5927)\t0.288675134595\n",
      "  (3107, 866)\t0.288675134595\n",
      "  (3107, 3447)\t0.288675134595\n",
      "  (3107, 620)\t0.288675134595\n",
      "  (3107, 4397)\t0.288675134595\n",
      "  (3107, 3512)\t0.288675134595\n",
      "  (3107, 3494)\t0.288675134595\n",
      "  (3107, 677)\t0.288675134595\n",
      "\n",
      "  (0, 6079)\t0.214096699844\n",
      "  (0, 7127)\t0.276131681956\n",
      "  (0, 5644)\t0.256639657848\n",
      "  (0, 677)\t0.0470888689476\n",
      "  (0, 6056)\t0.342563938898\n",
      "  (0, 3333)\t0.118856024916\n",
      "  (0, 2779)\t0.282779486517\n",
      "  (0, 3494)\t0.118218299927\n",
      "  (0, 1537)\t0.342563938898\n",
      "  (0, 4401)\t0.300265317902\n",
      "  (0, 6083)\t0.282779486517\n",
      "  (0, 3198)\t0.138165888316\n",
      "  (0, 1475)\t0.136312039941\n",
      "  (0, 3669)\t0.360049770283\n",
      "  (0, 2123)\t0.360049770283\n",
      "  (1, 677)\t0.0479787403513\n",
      "  (1, 4852)\t0.366853883477\n",
      "  (1, 7222)\t0.28134993614\n",
      "  (1, 3512)\t0.130968545513\n",
      "  (1, 4543)\t0.186947004428\n",
      "  (1, 2637)\t0.276860346113\n",
      "  (1, 2328)\t0.300764216085\n",
      "  (1, 5935)\t0.300764216085\n",
      "  (1, 4397)\t0.142764034991\n",
      "  (1, 5970)\t0.349037610065\n",
      "  :\t:\n",
      "  (3106, 6942)\t0.17352684226\n",
      "  (3106, 654)\t0.212436825835\n",
      "  (3106, 521)\t0.225942847108\n",
      "  (3106, 6542)\t0.215376683498\n",
      "  (3106, 4469)\t0.230268854685\n",
      "  (3106, 900)\t0.281185524279\n",
      "  (3106, 2161)\t0.260631612373\n",
      "  (3106, 1692)\t0.255727189482\n",
      "  (3106, 2534)\t0.291751687889\n",
      "  (3106, 2243)\t0.281185524279\n",
      "  (3106, 2987)\t0.306643859076\n",
      "  (3106, 1388)\t0.306643859076\n",
      "  (3106, 4661)\t0.306643859076\n",
      "  (3107, 677)\t0.0597479818935\n",
      "  (3107, 3494)\t0.1499994585\n",
      "  (3107, 3512)\t0.16309528405\n",
      "  (3107, 4397)\t0.177784220995\n",
      "  (3107, 620)\t0.173301801805\n",
      "  (3107, 3447)\t0.264695482852\n",
      "  (3107, 866)\t0.343058708971\n",
      "  (3107, 5927)\t0.231454170984\n",
      "  (3107, 7144)\t0.323200900247\n",
      "  (3107, 1361)\t0.418915285233\n",
      "  (3107, 2854)\t0.43465694709\n",
      "  (3107, 3503)\t0.418915285233\n"
     ]
    }
   ],
   "source": [
    "print(tf_matrix)\n",
    "print()\n",
    "print(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-bd7fd57ffd36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tokens(text):\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # sacar numeros, puntuaciones, etc.\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tokens(text[300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se transforma todas tweets a valores numéricos (1,0)\n",
    "tweet2index = {}\n",
    "for i, tweet in enumerate(list(set([x for x in text]))):\n",
    "    tweet2index[tweet] = i\n",
    "    \n",
    "y = np.array([tweet2index[x] for x in text])\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtener la matriz TF-IDF de los tweets\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X_1 = TfidfVectorizer(tokenizer=clean_tokens, stop_words='english').fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(clf, X, y):\n",
    "    scores = cross_val_score(clf, X, y, cv=5)\n",
    "    print(\"%s accuracy: %0.2f (+/- %0.2f)\" % \\\n",
    "          (str(clf.__class__).split('.')[-1].replace('>','').replace(\"'\",''), \n",
    "          scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_models(X, y):\n",
    "    run_model(SGDClassifier(), X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_1.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_models(X_1, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
